import pandas as pd
import numpy as np
import os
import argparse
import logging
import glob
import ast # For safely evaluating string representation of list
from tqdm import tqdm
import sys # For path manipulation

# --- Add project root to sys.path for dictionary import --- 
# Determine the directory of the current script
script_dir = os.path.dirname(os.path.abspath(__file__))
# Go one level up to reach the project root (assuming dashboard_pipeline is in the root)
project_root_dir = os.path.dirname(script_dir)
sys.path.insert(0, project_root_dir)

# --- Import the classification dictionary ---
try:
    from dictionaries.un_classification import un_classification
    logging.info("Successfully imported 'un_classification' dictionary.")
    # Pre-extract main category keys for faster lookup
    main_category_keys = set(un_classification.keys())
except ImportError:
    logging.error("ERROR: Could not import 'un_classification' from dictionaries.")
    logging.error(f"Ensure 'dictionaries/un_classification.py' exists in project root: {project_root_dir}")
    un_classification = None
    main_category_keys = set()
except Exception as e:
    logging.error(f"ERROR: An unexpected error occurred during dictionary import: {e}")
    un_classification = None
    main_category_keys = set()

# --- Configuration ---
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

# --- Helper Functions ---
def find_latest_input_csv(pipeline_output_dir):
    """Finds the most recent raw voting data CSV file."""
    pattern = os.path.join(pipeline_output_dir, 'UN_VOTING_DATA_RAW_WITH_TAGS_*.csv')
    list_of_files = glob.glob(pattern)
    if not list_of_files:
        logging.warning(f"No CSV files found in '{pipeline_output_dir}' matching the pattern.")
        return None
    try:
        latest_file = max(list_of_files, key=os.path.getmtime)
        logging.info(f"Found latest input file: {latest_file}")
        return latest_file
    except Exception as e:
        logging.error(f"Error determining latest file: {e}. Please specify the input file manually.")
        return None

def identify_country_columns(df_columns):
    """Identifies likely country ISO3 columns (3 uppercase letters)."""
    potential_countries = []
    for col in df_columns:
        if isinstance(col, str) and len(col) == 3 and col.isupper():
            potential_countries.append(col)
    known_non_countries = {'YES', 'NO'}
    country_cols = sorted([col for col in potential_countries if col not in known_non_countries])
    # No logging here, done in main function if needed
    return country_cols

def parse_tags_for_subtag1(tag_string):
    """
    Parses the comma-separated tag string generated by the upstream script
    and extracts the first subtag associated with each identified main category.
    If no valid subtag1 follows a main category, the main category itself is used.
    If no main categories are found in a non-empty tag string, returns ["No Tag"].

    Relies on the global 'un_classification' dictionary and 'main_category_keys' set.

    Returns a list of unique tags found (subtag1 or main category or "No Tag").
    """
    # Check dependencies and handle invalid inputs
    if un_classification is None: return []
    if pd.isna(tag_string) or not isinstance(tag_string, str) or tag_string.lower() == 'nan': return []

    # Split and clean the input string, removing empty items
    tag_items = [item.strip() for item in tag_string.split(',') if item.strip()]
    if not tag_items: return [] # Return empty if string was just whitespace/commas

    found_tags_for_this_vote = [] # Store tags (subtag1 or main) identified for this specific vote string
    i = 0
    while i < len(tag_items):
        current_item = tag_items[i]

        # Is the current item a main category?
        if current_item in main_category_keys:
            main_category = current_item
            tag_to_add = main_category # Default: use the main category itself

            # Look ahead: Does a next item exist?
            if i + 1 < len(tag_items):
                next_item = tag_items[i+1]
                # Is the next item a valid subtag1 for the current main category?
                if next_item in un_classification.get(main_category, {}):
                    tag_to_add = next_item # Override: use the subtag1
                    i += 1 # Increment extra to skip this subtag in the next outer loop iteration

            # Add the determined tag (either subtag1 or the main category fallback)
            found_tags_for_this_vote.append(tag_to_add)

        # Always advance the main loop counter
        i += 1

    # Decide the final return value based on what was found
    if found_tags_for_this_vote:
        # We found main categories and derived tags (subtag1 or main itself)
        return list(set(found_tags_for_this_vote))
    else:
        # We iterated through the items, but none were recognized main categories.
        # Since we already checked tag_items is not empty, this means the original string had content.
        return ["No Tag"]

# --- Main Logic ---
def aggregate_topic_votes(input_csv_path, output_csv_path):
    """
    Aggregates votes yearly by country and the first subtag.

    Args:
        input_csv_path (str): Path to the raw voting data CSV.
        output_csv_path (str): Path to save the yearly topic vote aggregates.
    """
    # Check if dictionary loaded successfully
    if un_classification is None:
        logging.error("Cannot proceed without the un_classification dictionary.")
        return
        
    logging.info(f"Reading raw voting data from: {input_csv_path}")
    try:
        # Specify dtype for potentially mixed-type columns if known, otherwise low_memory=False
        df_raw = pd.read_csv(input_csv_path, low_memory=False)
        logging.info(f"Loaded {len(df_raw)} rows.")
    except FileNotFoundError:
        logging.error(f"ERROR: Input file not found at {input_csv_path}")
        return
    except Exception as e:
        logging.error(f"ERROR: Failed to load input CSV: {e}")
        return

    # --- Data Preparation ---
    # Ensure 'Year' column exists and is numeric
    if 'Year' not in df_raw.columns:
        logging.warning("'Year' column not found, attempting to extract from 'Date'.")
        if 'Date' in df_raw.columns:
             try:
                 df_raw['Year'] = pd.to_datetime(df_raw['Date'], errors='coerce').dt.year
             except Exception as date_err:
                 logging.error(f"Error parsing 'Date' column: {date_err}")
                 return
        else:
             logging.error("Cannot determine year. Missing 'Year' and 'Date' columns.")
             return
    else:
        df_raw['Year'] = pd.to_numeric(df_raw['Year'], errors='coerce')

    df_raw.dropna(subset=['Year'], inplace=True)
    df_raw['Year'] = df_raw['Year'].astype(int)

    # Identify country columns
    country_cols = identify_country_columns(df_raw.columns)
    if not country_cols:
        logging.error("ERROR: No country columns identified. Cannot aggregate votes.")
        return
    logging.info(f"Identified {len(country_cols)} country columns for melting.")

    # Ensure 'tags' column exists
    if 'tags' not in df_raw.columns:
        logging.error("ERROR: 'tags' column not found. Cannot aggregate by topic.")
        return

    # --- Melting Data ---
    logging.info("Melting dataframe...")
    id_vars = ['Year', 'Resolution', 'tags'] # Keep Resolution for potential unique vote id
    # Add other potentially useful metadata if needed
    id_vars = [col for col in id_vars if col in df_raw.columns]

    df_melted = df_raw.melt(
        id_vars=id_vars,
        value_vars=country_cols,
        var_name='Country',
        value_name='Vote'
    )
    logging.info(f"Melted dataframe shape: {df_melted.shape}")

    # Filter out non-votes before parsing tags (efficiency)
    df_melted = df_melted[df_melted['Vote'].isin(['YES', 'NO', 'ABSTAIN'])]
    if df_melted.empty:
        logging.warning("No valid votes (YES/NO/ABSTAIN) found after melting.")
        return
    logging.info(f"Filtered for valid votes. Shape: {df_melted.shape}")

    # --- Tag Parsing and Exploding ---
    logging.info("Parsing tags and exploding dataframe...")
    # Ensure tags are strings before applying parse function
    df_melted['tags'] = df_melted['tags'].astype(str)
    tqdm.pandas(desc="Parsing Tags")
    df_melted['TopicTags'] = df_melted['tags'].progress_apply(parse_tags_for_subtag1)

    # Explode the dataframe based on the list of topic tags
    # This creates a row for each Country-Vote-TopicTag combination
    df_exploded = df_melted.explode('TopicTags')
    df_exploded.dropna(subset=['TopicTags'], inplace=True) # Remove rows where parsing failed or returned empty
    if df_exploded.empty:
        # *** This might still happen if the parsing logic returns [] for all rows ***
        # *** Let's add a check here to see what TopicTags contains before dropping/exploding ***
        non_empty_tags = df_melted[df_melted['TopicTags'].apply(lambda x: isinstance(x, list) and len(x) > 0)]
        if non_empty_tags.empty:
             logging.warning("The 'parse_tags_for_subtag1' function did not return any valid subtags for any row. Check the function logic and input data format again.")
             # Log a sample of input tags and their parsed (empty) results
             sample_tags = df_melted['tags'].head(20).tolist()
             sample_parsed = [parse_tags_for_subtag1(t) for t in sample_tags]
             logging.warning(f"Sample input tags: {sample_tags}")
             logging.warning(f"Sample parsed results: {sample_parsed}")
        else:
             logging.warning("Dataframe is empty after exploding tags, but some rows had non-empty TopicTags lists before exploding. This is unexpected.")
        return # Exit if empty after explode
        
    logging.info(f"Exploded dataframe shape: {df_exploded.shape}")

    # --- Aggregation ---
    logging.info("Grouping and counting votes by Year, Country, TopicTag...")
    grouping_cols = ['Year', 'Country', 'TopicTags', 'Vote']
    df_counts = df_exploded.groupby(grouping_cols).size().unstack(fill_value=0)

    # Ensure all vote columns exist
    for vote_type in ['YES', 'NO', 'ABSTAIN']:
        if vote_type not in df_counts.columns:
            df_counts[vote_type] = 0

    # Rename columns
    df_counts = df_counts.rename(columns={
        'YES': 'YesVotes_Topic',
        'NO': 'NoVotes_Topic',
        'ABSTAIN': 'AbstainVotes_Topic'
    })

    # Calculate total votes for the topic
    df_counts['TotalVotes_Topic'] = df_counts[['YesVotes_Topic', 'NoVotes_Topic', 'AbstainVotes_Topic']].sum(axis=1)

    # Reset index to bring grouping keys back as columns
    df_final = df_counts.reset_index()

    # Rename 'TopicTags' to 'TopicTag' for clarity
    df_final = df_final.rename(columns={'TopicTags': 'TopicTag'})

    # Reorder columns
    final_cols_order = ['Year', 'Country', 'TopicTag', 'YesVotes_Topic', 'NoVotes_Topic', 'AbstainVotes_Topic', 'TotalVotes_Topic']
    df_final = df_final[final_cols_order]

    # --- Save Output ---
    output_dir = os.path.dirname(output_csv_path)
    if output_dir and not os.path.exists(output_dir):
        logging.info(f"Creating output directory: {output_dir}")
        os.makedirs(output_dir)

    logging.info(f"Saving aggregated topic vote results to: {output_csv_path}")
    try:
        df_final.to_csv(output_csv_path, index=False)
        logging.info(f"Successfully saved {len(df_final)} rows.")
    except Exception as e:
        logging.error(f"ERROR: Failed to save output CSV: {e}")

# --- Main Execution ---
if __name__ == "__main__":
    # Determine script's directory to build relative paths
    script_dir = os.path.dirname(os.path.abspath(__file__))
    
    parser = argparse.ArgumentParser(description="Aggregate UN votes yearly by country and first subtag.")
    # Keep -p argument for flexibility in specifying where the raw data is
    parser.add_argument("-p", "--pipeline-output-dir", default="../pipeline_output",
                        help="Directory containing the raw voting data CSVs (relative to script location, default: ../pipeline_output)")

    args = parser.parse_args()

    # Construct absolute path for pipeline_output_dir relative to script location
    pipeline_output_dir_abs = os.path.abspath(os.path.join(script_dir, args.pipeline_output_dir))

    # Determine input file path using the absolute path
    input_path = find_latest_input_csv(pipeline_output_dir_abs)

    # Determine fixed output path relative to script location
    output_filename = "topic_votes_yearly.csv"
    output_path = os.path.abspath(os.path.join(script_dir, "output", output_filename))

    if not input_path:
        logging.error("No input file specified or found. Exiting.")
    else:
        aggregate_topic_votes(input_path, output_path)
        logging.info("Script finished.") 